{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PtF0VG3OHOeN","executionInfo":{"status":"ok","timestamp":1711949099894,"user_tz":240,"elapsed":912,"user":{"displayName":"sudharshan sundar","userId":"14830100596079146031"}},"outputId":"a6fe0478-22e6-4a5f-9f60-910c108884eb"},"id":"PtF0VG3OHOeN","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Colab Notebooks/NLP Assignment 2/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1YgwZKvjHSe6","executionInfo":{"status":"ok","timestamp":1711949130419,"user_tz":240,"elapsed":185,"user":{"displayName":"sudharshan sundar","userId":"14830100596079146031"}},"outputId":"270f08c3-3df4-412b-beb1-cf8e504320b4"},"id":"1YgwZKvjHSe6","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/NLP Assignment 2\n"]}]},{"cell_type":"code","source":["!pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mfO8ZdlNG80M","executionInfo":{"status":"ok","timestamp":1711949158051,"user_tz":240,"elapsed":26101,"user":{"displayName":"sudharshan sundar","userId":"14830100596079146031"}},"outputId":"4e45b319-782e-4404-be0a-d75143f520d7"},"id":"mfO8ZdlNG80M","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (4.3.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (3.7.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (3.8.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.25.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.5.3)\n","Collecting pytrec-eval (from -r requirements.txt (line 6))\n","  Downloading pytrec_eval-0.5.tar.gz (15 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.11.4)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (0.13.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (4.66.2)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->-r requirements.txt (line 1)) (6.4.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (4.50.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (24.0)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 2)) (2.8.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 3)) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 3)) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 3)) (2023.12.25)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 5)) (2023.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (3.4.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 2)) (1.16.0)\n","Building wheels for collected packages: pytrec-eval\n","  Building wheel for pytrec-eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pytrec-eval: filename=pytrec_eval-0.5-cp310-cp310-linux_x86_64.whl size=308202 sha256=bd8caa05b6c363744c225e0c180fc745b2964ade215d9c7695fafa780d9c13af\n","  Stored in directory: /root/.cache/pip/wheels/51/3a/cd/dcc1ddfc763987d5cb237165d8ac249aa98a23ab90f67317a8\n","Successfully built pytrec-eval\n","Installing collected packages: pytrec-eval\n","Successfully installed pytrec-eval-0.5\n"]}]},{"cell_type":"code","execution_count":11,"id":"bf7c47c6-30ab-4a53-8419-bfe32f15cbbd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bf7c47c6-30ab-4a53-8419-bfe32f15cbbd","executionInfo":{"status":"ok","timestamp":1711949577442,"user_tz":240,"elapsed":71436,"user":{"displayName":"sudharshan sundar","userId":"14830100596079146031"}},"outputId":"fcb4fe9f-55ca-4ce8-9eef-44df2d272de6"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["######## Simlex - loaded dataframe########\n","   word1        word2 SimLex999\n","0    old          new      1.58\n","1  smart  intelligent       9.2\n","2   hard    difficult      8.77\n","3  happy     cheerful      9.55\n","4   hard         easy      0.95\n"]},{"output_type":"stream","name":"stderr","text":["Combining outputs: 100%|██████████| 616/616 [00:00<00:00, 41787.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["######## Tfidf - model########\n","Tfidf best min_df:  0.0005\n","Tfidf scores:  [0.00723051 0.0054856  0.00315596 0.00530336]\n","Tfidf vocab sizes:  [2021, 1119, 155, 54]\n","######## Word2Vec - model########\n","Word2Vec best window:  5\n","Word2Vec best vector size:  150\n","Word2Vec scores:  [[0.00346362 0.00293392 0.00240323 0.00267763]\n"," [0.00276972 0.00334003 0.00341921 0.00242605]\n"," [0.00346204 0.00160937 0.00566724 0.00208077]\n"," [0.001667   0.00145449 0.00285823 0.0044895 ]]\n"]}],"source":["import numpy as np\n","import os\n","import os.path as osp\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from scipy import sparse\n","import pickle\n","import pytrec_eval\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from gensim.models import Word2Vec\n","import nltk\n","from nltk.corpus import brown\n","from nltk.tokenize import word_tokenize\n","from tqdm import tqdm\n","from concurrent.futures import ProcessPoolExecutor\n","\n","nltk.download('brown')\n","\n","class TfidfTextContextSimilarWord:\n","    def __init__(self, text: list):\n","        self.text = text\n","        self.X = sparse.csr_matrix(np.array([]))\n","        self.vocabulary = []\n","\n","    def tune_min_df(self, min_df: list, val_list_of_words: list, evaluator):\n","        best_result = -1\n","        best_min_df = None\n","        scores = np.zeros((len(min_df)))\n","        vocab_sizes = []\n","\n","        selection_parameter = 'ndcg average'\n","        for i, min_df_ in enumerate(min_df):\n","            self.fit(min_df_)\n","            predictions = self.predict(val_list_of_words)\n","            predictions = prepare_dict_for_evaluation(predictions)\n","\n","            vocab_sizes.append(len(self.vocabulary))\n","\n","            m_s = evaluator.evaluate(predictions)[selection_parameter]\n","            if m_s > best_result:\n","                best_result = m_s\n","                best_min_df = min_df_\n","            scores[i] = m_s\n","        return best_min_df, scores, vocab_sizes\n","\n","    def fit(self, min_df: float):\n","        vectorizer = TfidfVectorizer(min_df=min_df, stop_words='english', ngram_range=(1, 1))\n","        X = vectorizer.fit_transform(self.text)\n","\n","        self.X = X.T\n","        self.X = sparse.csr_matrix(self.X)\n","        self.vocabulary = list(vectorizer.get_feature_names_out())\n","\n","    def predict(self, list_of_words):\n","        tfidf_similar = {}\n","        for i in list_of_words:\n","            ix = [i_ for i_, w in enumerate(self.vocabulary) if w == i]\n","            if len(ix) == 1:\n","                ix_ = ix[0]\n","                w1 = self.X[ix_]\n","                sim_scores = [(w, p) for w, p in zip(self.vocabulary,\n","                                                     list(enumerate(cosine_similarity(w1, self.X)))[0][1])]\n","                sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n","                sim_scores = sim_scores[1:11]\n","\n","                dict_ = {}\n","                for elem in sim_scores:\n","                    dict_[elem[0]] = elem[1]\n","                    tfidf_similar[i] = dict_\n","            else:\n","                pass\n","        return tfidf_similar\n","\n","    def save_model(self, model_loc: str):\n","        if os.path.exists(model_loc):\n","            save_sparse_csr(osp.join(model_loc, 'tfidf_word_context.pkl'), self.X)\n","            with open(osp.join(model_loc, 'tfidf_vocabulary.txt'), \"w\") as file:\n","                file.write(\"\\n\".join(self.vocabulary))  # Write each word in a new line\n","        else:\n","            try:\n","                os.makedirs(model_loc, exist_ok=True)\n","                save_sparse_csr(osp.join(model_loc, 'tfidf_word_context.pkl'), self.X)\n","                with open(osp.join(model_loc, 'tfidf_vocabulary.txt'), \"w\") as file:\n","                    file.write(\"\\n\".join(self.vocabulary))  # Write each word in a new line\n","            except TypeError:\n","                print('Cant save the model !')\n","\n","    def load_model(self, model_loc: str):\n","        if os.path.exists(model_loc):\n","            self.X = load_sparse_csr(osp.join(model_loc, 'tfidf_word_context.pkl'))\n","            with open(osp.join(model_loc, 'tfidf_vocabulary.txt'), \"r\") as file:\n","                self.vocabulary = file.read().splitlines()\n","        else:\n","            raise Exception('Cant load the model !')\n","\n","class Word2VecSimilarWord:\n","    def __init__(self, text):\n","        self.text = text\n","        self.model = Word2Vec()\n","\n","    def tune_window_n_size(self, windows: list, vector_sizes: list, val_list_of_words: list,\n","                           evaluator):\n","        best_result = -1\n","        best_window = None\n","        best_vector_size = None\n","\n","        scores = np.zeros((len(windows), len(vector_sizes)))\n","        selection_parameter = 'ndcg average'\n","        for i, window in enumerate(windows):\n","            for j, vector_size in enumerate(vector_sizes):\n","                self.fit(window, vector_size)\n","                predictions = self.predict(val_list_of_words)\n","                predictions = prepare_dict_for_evaluation(predictions)\n","\n","                m_s = evaluator.evaluate(predictions)[selection_parameter]\n","                if m_s > best_result:\n","                    best_result = m_s\n","                    best_window = window\n","                    best_vector_size = vector_size\n","                scores[i][j] = m_s\n","        return best_window, best_vector_size, scores\n","\n","    def fit(self, window: int, vector_size: int):\n","        try:\n","            self.model = Word2Vec(sentences=self.text, window=window, min_count=1, vector_size=vector_size,\n","                                  epochs=10)\n","        except DeprecationWarning:\n","            self.model = Word2Vec(sentences=self.text, window=window, min_count=1, size=vector_size,\n","                                  epochs=10)\n","\n","    def save_model(self, model_loc: str):\n","        if os.path.exists(model_loc):\n","            self.model.save(osp.join(model_loc, \"word2vec.model\"))\n","        else:\n","            try:\n","                os.makedirs(model_loc, exist_ok=True)\n","                self.model.save(osp.join(model_loc, \"word2vec.model\"))\n","            except TypeError:\n","                print('Cant save the model !')\n","\n","\n","    def load_model(self, model_loc: str):\n","        if osp.exists(model_loc):\n","            self.model = Word2Vec.load(osp.join(model_loc, \"word2vec.model\"))\n","        else:\n","            raise Exception('Cant load the model !')\n","\n","    def predict(self, list_of_words: list):\n","        try:\n","            vocab = list(self.model.wv.index_to_key)\n","        except DeprecationWarning:\n","            vocab = list(self.model.wv.vocab.keys())\n","        word2vec_similar = {}\n","        for i in list_of_words:\n","            if i in vocab:\n","                sims = self.model.wv.most_similar(i, topn=10)\n","                dict_ = {}\n","                for elem in sims:\n","                    dict_[elem[0]] = elem[1]\n","                word2vec_similar[i] = dict_\n","            else:\n","                pass\n","        return word2vec_similar\n","\n","class DataLoader:\n","    def __init__(self):\n","        self.corpus_allowed = ['romance', 'news']\n","\n","    def fetch_data(self, corpus: str = ''):\n","        if corpus in self.corpus_allowed:\n","            if corpus == 'romance':\n","                tokenized_sentences = brown.sents(categories='romance')\n","                sentences = [' '.join(sent) for sent in brown.sents(categories='romance')]\n","                return tokenized_sentences, sentences\n","            elif corpus == 'news':\n","                tokenized_sentences = brown.sents(categories='news')\n","                sentences = [' '.join(sent) for sent in brown.sents(categories='news')]\n","                return tokenized_sentences, sentences\n","        else:\n","            raise Exception('Corpus not in allowed corpuses')\n","\n","class SimLexLoader:\n","    def __init__(self, file_loc: str = \"Dataset/SimLex-999.txt\"):\n","        self.file_loc = file_loc\n","\n","    def load(self):\n","        result1 = [x.split('\\t')[0] for x in open(self.file_loc).readlines()]\n","        result2 = [x.split('\\t')[1] for x in open(self.file_loc).readlines()]\n","        result3 = [x.split('\\t')[3] for x in open(self.file_loc).readlines()]\n","\n","        result1.remove('word1')\n","        result2.remove('word2')\n","        result3.remove('SimLex999')\n","\n","        self.df = pd.DataFrame(list(zip(result1, result2, result3)),\n","                       columns =['word1', 'word2', 'SimLex999'])\n","        return self.df\n","\n","    @staticmethod\n","    def return_one_level_similar_words(temp, i_sim, result_dict):\n","        i_sim_old = i_sim\n","        for j in list(i_sim.keys()):\n","            if j in temp:\n","                j_dict = result_dict.get(j)\n","                for k in list(j_dict.keys()):\n","                    if k not in list(i_sim.keys()):\n","                        i_sim.update({k: j_dict[k]})\n","        return i_sim, i_sim_old\n","\n","    def get_top_words(self):\n","        result_dict = {}\n","\n","        for word1, word2, simi in zip(self.df['word1'], self.df['word2'], self.df['SimLex999']):\n","            if word1 not in result_dict.keys():\n","                result_dict[word1] = {word2: simi}\n","            else:\n","                if word2 not in result_dict.get(word1):\n","                    result_dict[word1].update({word2: simi})\n","\n","        temp = result_dict.keys()\n","\n","        for i in tqdm(temp, desc='Combining outputs: '):\n","            i_sim = result_dict.get(i)\n","            i_sim_old = {}\n","            while len(i_sim) < 10 and list(i_sim.keys()) != list(i_sim_old.keys()):\n","                i_sim, i_sim_old = self.return_one_level_similar_words(temp, i_sim, result_dict)\n","            result_dict[i] = i_sim\n","\n","        final_dict = dict()\n","\n","        for key, val in result_dict.items():\n","            val = {k: float(v) for k, v in val.items()}\n","            sorted_val = {k: v for k, v in sorted(val.items(), key=lambda item: item[1], reverse=True)}\n","            final_dict[key] = sorted_val\n","\n","        return final_dict\n","\n","def save_sparse_csr(filename, array):\n","    array = array.todense()\n","    with open(filename, 'wb') as f:\n","        pickle.dump(array, f)\n","\n","def load_sparse_csr(filename):\n","    with open(filename, 'rb') as f:\n","        x = pickle.load(f)\n","    return x\n","\n","def prepare_dict_for_evaluation(res):\n","    res_for_eval = {}\n","    for k in res.keys():\n","        dict_ = {}\n","        sorted_gt = [(k, v) for k, v in sorted(res.get(k).items(), key=lambda item: item[1])]\n","        for idx, l in enumerate(sorted_gt[:10]):\n","            dict_[l[0]] = 1\n","        res_for_eval[k] = dict_\n","    return res_for_eval\n","\n","class PyTrecEvaluator:\n","    def __init__(self, res_golden):\n","        self.evaluator = pytrec_eval.RelevanceEvaluator(res_golden, {'ndcg'})\n","\n","    def evaluate(self, result_evaluation: dict):\n","        result = self.evaluator.evaluate(result_evaluation)\n","\n","        metrics = {}\n","        for measure in sorted(list(result[list(result.keys())[0]].keys())):\n","            metrics[f'{measure} average'] = \\\n","                pytrec_eval.compute_aggregated_measure(\n","                    measure, [query_measures[measure] for query_measures in result.values()])\n","        return metrics\n","\n","def hyper_parameter_CM(array, index: list, columns: list, title: str,\n","                       xlabel: str, ylabel: str, img_loc: str):\n","    df_cm = pd.DataFrame(array, index=index, columns=columns)\n","    plt.figure(figsize=(10, 7))\n","    sns.heatmap(df_cm, annot=True, cmap='gist_earth_r')\n","    plt.xlabel(xlabel)\n","    plt.ylabel(ylabel)\n","    plt.title(title)\n","    plt.savefig(img_loc)\n","    plt.close()\n","\n","def line_plots(arrays: list, labels: list, xticks: list, xlabel: str,\n","               ylabel: str, title: str, img_loc: str):\n","    if len(arrays) == 2:\n","        fig, ax1 = plt.subplots()\n","\n","        ax2 = ax1.twinx()\n","        ax1.plot(arrays[0], 'g-', label=labels[0])\n","        ax2.plot(arrays[1], 'b-', label=labels[0])\n","        ax1.legend()\n","        ax2.legend()\n","        ax1.set_xlabel(xlabel)\n","        ax1.set_ylabel(labels[0], color='g')\n","        ax2.set_ylabel(labels[1], color='b')\n","        plt.title(title)\n","    else:\n","        for array, label in zip(arrays, labels):\n","            plt.plot(array, label=label)\n","        plt.legend()\n","        plt.xlabel(xlabel)\n","        plt.ylabel(ylabel)\n","        plt.title(title)\n","    plt.xticks(range(len(arrays[0])), xticks)\n","    plt.savefig(img_loc, bbox_inches='tight')\n","    plt.close()\n","\n","class Experiment:\n","    def __init__(self, model_loc: str, output_loc: str, corpus: str, file_loc: str):\n","        self.model_loc = model_loc\n","        self.output_loc = output_loc\n","        self.corpus = corpus\n","        self.file_loc = file_loc\n","\n","    def load_simlex_data(self):\n","        simlex_loader = SimLexLoader(file_loc=self.file_loc)\n","        df = simlex_loader.load()\n","        print('#' * 8 + ' Simlex - loaded dataframe' + '#' * 8)\n","        print(df.head())\n","        self.result_golden = simlex_loader.get_top_words()\n","        self.test_words = list(self.result_golden.keys())\n","\n","    def initiate_evaluator(self):\n","        self.result_golden_for_evaluator = prepare_dict_for_evaluation(self.result_golden)\n","        self.evaluator = PyTrecEvaluator(res_golden=self.result_golden_for_evaluator)\n","\n","    def load_data(self):\n","        loader = DataLoader()\n","        self.tokenized_sentences, self.sentences = loader.fetch_data(self.corpus)\n","\n","    def tfidf(self):\n","        print('#' * 8 + ' Tfidf - model' + '#' * 8)\n","        min_df = [0.0005, 0.001, 0.005, 0.01]\n","        tfidf = TfidfTextContextSimilarWord(self.sentences)\n","        best_min_df, scores, vocab_sizes = tfidf.tune_min_df(min_df=min_df,\n","                                                              val_list_of_words=self.test_words,\n","                                                              evaluator=self.evaluator)\n","        print('Tfidf best min_df: ', best_min_df)\n","        print('Tfidf scores: ', scores)\n","        print('Tfidf vocab sizes: ', vocab_sizes)\n","        hyper_parameter_CM(scores, min_df, ['ndcg'], 'Tfidf hyper-parameter tuning',\n","                           'min_df', 'ndcg', osp.join(self.output_loc, 'tfidf_hyper_param.png'))\n","        tfidf.save_model(osp.join(self.model_loc, 'tfidf_best_model'))\n","\n","    def word2vec(self):\n","        print('#' * 8 + ' Word2Vec - model' + '#' * 8)\n","        windows = [2, 3, 5, 7]\n","        vector_sizes = [50, 100, 150, 200]\n","        word2vec = Word2VecSimilarWord(self.tokenized_sentences)\n","        best_window, best_vector_size, scores = word2vec.tune_window_n_size(windows=windows,\n","                                                                            vector_sizes=vector_sizes,\n","                                                                            val_list_of_words=self.test_words,\n","                                                                            evaluator=self.evaluator)\n","        print('Word2Vec best window: ', best_window)\n","        print('Word2Vec best vector size: ', best_vector_size)\n","        print('Word2Vec scores: ', scores)\n","        hyper_parameter_CM(scores, windows, vector_sizes, 'Word2Vec hyper-parameter tuning',\n","                           'window', 'vector size',\n","                           osp.join(self.output_loc, 'word2vec_hyper_param.png'))\n","        word2vec.save_model(osp.join(self.model_loc, 'word2vec_best_model'))\n","\n","    def run(self):\n","        self.load_simlex_data()\n","        self.initiate_evaluator()\n","        self.load_data()\n","        self.tfidf()\n","        self.word2vec()\n","\n","if __name__ == '__main__':\n","    model_loc = 'model'\n","    output_loc = 'output'\n","    corpus = 'romance'\n","    file_loc = 'Dataset/SimLex-999.txt'\n","    experiment = Experiment(model_loc=model_loc, output_loc=output_loc, corpus=corpus, file_loc=file_loc)\n","    experiment.run()\n"]},{"cell_type":"code","source":[],"metadata":{"id":"l5CVTnBtIiRJ"},"id":"l5CVTnBtIiRJ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"applied-ai","language":"python","name":"applied-ai"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}